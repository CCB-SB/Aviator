from glob import glob
from os.path import basename, splitext
from datetime import date

BATCHES = 13
rule all:
    input: 
        "results/query_table.filtered.csv",
        "results/biotools.filtered.pubmed_info.csv",
        "results/nar_ws.pubmed_info.csv",
        "results/wos_results_filtered_pmids",
        "results/publication_table_for_webservice.csv",
        "results/new2orig_urls_all_previous.json",
        expand("download/urls2query.{n}.csv", n=range(1, BATCHES + 1)),
	    "results/biotoolsid2pmid.csv"

rule get_query_table:
    output:
        csv="results/query_table.csv"
    conda: "envs/biopython.yml"
    script: "scripts/get_query_table.py"

rule parse_urls_java:
    input: "results/{tbl}.csv"
    output: "results/{tbl}.java.csv"
    shell: "java -jar bin/abstract_url_parser.jar {input} {output}"

rule parse_urls_python:
    input: 
        query="results/{tbl}.csv",
        java_urls="results/{tbl}.java.csv"
    output: 
        csv="results/{tbl}.java_python_urls.csv"
    conda: "envs/biopython.yml"
    script: "scripts/add_python_urls.py"

rule filter_biotools:
    input:
        biotools="results/biotools.json",
        doi2pmid="data/doi2pmid.csv"
    output: "results/biotools.filtered.csv"
    script: "scripts/filter_biotools.R"

rule dl_biotools:
    output: 
        pkl="results/biotools.pkl",
        json="results/biotools.json"
    script: "scripts/download_biotools_web.py"

rule get_biotools_pubmed_tbl:
    input: 
        biotools="results/biotools.filtered.csv"
    output: 
        csv="results/biotools.filtered.pubmed_info.csv"
    conda: "envs/biopython.yml"
    script: "scripts/get_biotools_pubmed_info.py"

rule get_biotoolsid2pmid:
    input:
        filtered="results/biotools.filtered.csv",
        json="results/biotools.json"
    output:
        "results/biotoolsid2pmid.csv"
    script: "scripts/add_biotools_id.R"

rule collect_nar_ws_pubmed_tbl:
    input:
        nar="data/nar_webserver_issue_articles_since_2010.csv"
    output:
        csv="results/nar_ws.pubmed_info.csv"
    conda: "envs/biopython.yml"
    script: "scripts/get_nar_ws_pubmed_info.py"

rule collected_curated_url_pubmed_tbl:
    input: "data/curated_urls.csv"
    output: 
        csv="results/curated_urls.pubmed_info.csv"
    conda: "envs/biopython.yml"
    script: "scripts/get_curated_urls_pubmed_info.py"

rule filter_queries:
    input: 
        pubmed="results/query_table.java_python_urls.csv",
        biotools="results/biotools.filtered.pubmed_info.java_python_urls.csv",
        nar="results/nar_ws.pubmed_info.java_python_urls.csv",
        nar_manual_cur="data/nar_manual_curated_urls.csv",
        no_ws_list="data/pmids_not_webservers.txt",
        manual_cur="data/curated_urls.csv",
        manual_cur_pubmed="results/curated_urls.pubmed_info.csv"
    output: 
        filtered_tbl="results/query_table.filtered.csv",
        orig_url2new_id="results/orig2new_urls_and_id.csv",
        urls2query="results/urls2query.csv",
        urls2query_batches=expand("results/urls2query.{n}.csv", n=range(1, BATCHES + 1))
    params:
        batches=BATCHES
    conda: "envs/R.yml"
    threads: 4
    script: "scripts/filter_queries.R"

rule old_orig2new_url_map:
    input:
        glob("data/old_data/orig2new_urls*.csv")
    output:
        "results/new2orig_urls_all_previous.json"
    conda: "envs/biopython.yml"
    script: "scripts/create_previous_orig2new_url_map.py"

rule download_wos_info:
    input: "results/query_table.filtered.csv"
    output: directory("results/wos_results_filtered_pmids")
    conda: "envs/selenium.yml"
    shell: "python src/wos_downloader.py {input} {output}"

rule concat_wos_records:
    input: "results/wos_results_filtered_pmids"
    output: "results/wos_results_filtered_pmids_savedrecs.all.txt"
    shell: "cat {input}/savedrecs_* > {output}"

rule wos_pmid_mails:
    input: "results/wos_results_filtered_pmids_savedrecs.all.txt"
    output: "results/pmids_w_email_from_wos.csv"
    shell: "python src/extract_email.py {input} {output}"

rule mesh_terms:
    input: "results/query_table.filtered.csv"
    output: "results/pmids_w_pubmed_mesh_terms.csv"
    conda: "envs/biopython.yml"
    shell: "python src/get_pubmed_mesh_terms.py {input} {output}"

rule create_publication_table_for_ws:
    input:
        tbl="results/query_table.filtered.csv",
        emails="results/pmids_w_email_from_wos.csv",
        mesh_terms="results/pmids_w_pubmed_mesh_terms.csv"
    output: "results/publication_table_for_webservice.csv"
    conda: "envs/R.yml"    
    script: "scripts/create_pub_table_for_webserver.R"


rule download_folder:
    input: expand("results/urls2query.{n}.csv", n=range(1, BATCHES + 1))
    output: expand("download/urls2query.{n}.csv", n=range(1, BATCHES + 1))
    params: prefix="download"
    shell: "cp {input} {params.prefix}"

WS_FILES = [
    "urls2query.csv", "orig2new_urls_and_id.csv", "publication_table_for_webservice.csv",
    "new2orig_urls_all_previous.json", "biotoolsid2pmid.csv"
]

def add_timestamp(files):
    for f in files:
        base, ext = splitext(f)
        yield f"{base}.{date.today().isoformat()}{ext}"

rule copy_with_timestamp:
    input:
        "results/{file}.{ext}"
    output:
        f"../webserver/data/{{file}}.{date.today().isoformat()}.{{ext}}"
    shell: "cp {input} {output}"

rule copy_to_app:
    input: 
        expand("../webserver/data/{file}", file=add_timestamp(WS_FILES))

rule copy_old_ws_data:
    input: glob("../webserver/data/orig2new_urls_and_id*.csv")
    output: f"data/old_data/{basename(glob('../webserver/data/orig2new_urls_and_id*.csv')[-1])}"
    shell: "cp {input} {output}"
